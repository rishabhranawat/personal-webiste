  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
  </script>


<div class="post">

<blockquote>Reading Group: Asynchronous Methods for Deep Reinforcement Learning V Minh. et. al.</blockquote>


<p>The sequence of observed data encountered by an online RL agent is non-stationary, and online RL updates are strongly correlated. By storing the agent's data in an experience replay memory, the data can be batched or randomly sampled from different-steps.</p>

<i>Drawbacks of experience replay</i>


<h4>Asynchronous RL Framework</h4>

They present multi-threade asynchronous variants of one-step SARSA, one-step Q learning, n-step Q learning and advantage actor-critic.

Actor-critic being an on-policy search method and Q-learning being an off-policy value-based method


<i>Gorila Framework</i>
<i>Hogwild!</i>



</div>